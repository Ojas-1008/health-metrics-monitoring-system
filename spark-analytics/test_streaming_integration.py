"""
Integration test for micro-batch polling stream.

This script:
1. Authenticates with the backend API
2. Inserts a new health metric document
3. Monitors Spark logs to verify processing
4. Tests checkpoint resume functionality

Usage: python test_streaming_integration.py
"""

import requests
import time
from datetime import datetime, timedelta
import json
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
BACKEND_URL = os.getenv('BACKEND_API_URL', 'http://localhost:5000/api')
TEST_EMAIL = 'ojasshrivastava1008@gmail.com'
TEST_PASSWORD = 'Krishna@1008'

class BackendAPITester:
    def __init__(self, base_url, email, password):
        self.base_url = base_url
        self.email = email
        self.password = password
        self.token = None
        self.user_id = None
    
    def login(self):
        """Login and get JWT token"""
        print("ğŸ” Logging in to backend API...")
        
        url = f"{self.base_url}/auth/login"
        payload = {
            "email": self.email,
            "password": self.password
        }
        
        try:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            
            data = response.json()
            self.token = data.get('token')
            self.user_id = data.get('data', {}).get('user', {}).get('_id')
            
            print(f"âœ… Login successful!")
            print(f"   User ID: {self.user_id}")
            print(f"   Token: {self.token[:20]}...")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Login failed: {e}")
            if hasattr(e.response, 'text'):
                print(f"   Response: {e.response.text}")
            return False
    
    def insert_metric(self, date=None):
        """Insert a new health metric"""
        if not self.token:
            print("âŒ Not authenticated. Please login first.")
            return None
        
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        
        print(f"\nğŸ“Š Inserting health metric for date: {date}")
        
        url = f"{self.base_url}/metrics"
        headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "date": date,
            "metrics": {
                "steps": 8500,
                "distance": 6.2,
                "calories": 450,
                "activeMinutes": 65,
                "weight": 72.5,
                "sleepHours": 7.5
            },
            "source": "manual"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            print(f"âœ… Metric inserted successfully!")
            print(f"   Date: {date}")
            print(f"   Steps: {payload['metrics']['steps']}")
            print(f"   Timestamp: {datetime.now().isoformat()}")
            
            return data
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Failed to insert metric: {e}")
            if hasattr(e.response, 'text'):
                print(f"   Response: {e.response.text}")
            return None
    
    def get_metrics(self, start_date=None, end_date=None):
        """Get metrics for date range"""
        if not self.token:
            print("âŒ Not authenticated. Please login first.")
            return None
        
        if start_date is None:
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if end_date is None:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        print(f"\nğŸ“‹ Fetching metrics from {start_date} to {end_date}")
        
        url = f"{self.base_url}/metrics"
        headers = {
            "Authorization": f"Bearer {self.token}"
        }
        params = {
            "startDate": start_date,
            "endDate": end_date
        }
        
        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            
            data = response.json()
            count = data.get('count', 0)
            print(f"âœ… Retrieved {count} metrics")
            
            return data
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Failed to fetch metrics: {e}")
            return None


def check_checkpoint_file():
    """Check if checkpoint file exists and display timestamp"""
    checkpoint_dir = os.getenv('CHECKPOINT_LOCATION', './spark-checkpoints')
    checkpoint_file = os.path.join(checkpoint_dir, 'last_processed_timestamp.txt')
    
    print(f"\nğŸ“ Checking checkpoint file: {checkpoint_file}")
    
    if os.path.exists(checkpoint_file):
        with open(checkpoint_file, 'r') as f:
            timestamp = f.read().strip()
            print(f"âœ… Checkpoint exists: {timestamp}")
            return timestamp
    else:
        print(f"âš ï¸ No checkpoint file found (will default to 30 days ago)")
        return None


def display_test_instructions():
    """Display manual testing instructions"""
    print("\n" + "=" * 70)
    print("ğŸ§ª SPARK STREAMING INTEGRATION TEST")
    print("=" * 70)
    
    print("\nğŸ“‹ TEST PROCEDURE:")
    print("-" * 70)
    
    print("\nâœ… Step 1: Start Spark Streaming Job")
    print("   In a NEW PowerShell terminal, run:")
    print("   cd spark-analytics")
    print("   python main.py")
    print()
    print("   Expected output:")
    print("   - ğŸš€ Starting micro-batch polling stream")
    print("   - â±ï¸  Polling interval: 60 seconds")
    print("   - âœ… Streaming query started")
    print()
    
    print("âœ… Step 2: Wait for Initial Batch")
    print("   Wait for the first batch to complete (~60 seconds)")
    print("   Look for:")
    print("   - ğŸ”„ Starting micro-batch 0")
    print("   - ğŸ“ Loaded checkpoint timestamp (or using default)")
    print("   - â„¹ï¸ No new data found (if no recent updates)")
    print()
    
    print("âœ… Step 3: Insert New Metric (THIS SCRIPT)")
    print("   This script will insert a metric and show timestamp")
    print()
    
    print("âœ… Step 4: Wait for Next Batch")
    print("   Wait for next batch interval (~60 seconds)")
    print("   Look for:")
    print("   - ğŸ”„ Starting micro-batch 1 (or next number)")
    print("   - âœ… Found X new/updated records")
    print("   - ğŸ“ˆ Processing X health metrics records...")
    print("   - ğŸ’¾ Updating checkpoint to: [timestamp]")
    print()
    
    print("âœ… Step 5: Stop and Restart Spark Job")
    print("   Press Ctrl+C to stop the Spark job")
    print("   Restart with: python main.py")
    print("   Verify:")
    print("   - ğŸ“ Loaded checkpoint timestamp: [last saved timestamp]")
    print("   - Job resumes from correct position")
    print()
    
    print("-" * 70)
    print()


def run_integration_test():
    """Run the complete integration test"""
    
    display_test_instructions()
    
    print("ğŸš€ Starting Integration Test...")
    print("=" * 70)
    
    # Initialize API tester
    api = BackendAPITester(BACKEND_URL, TEST_EMAIL, TEST_PASSWORD)
    
    # Step 1: Login
    print("\n" + "=" * 70)
    print("STEP 1: Authenticate with Backend API")
    print("=" * 70)
    
    if not api.login():
        print("\nâŒ Test aborted - authentication failed")
        return False
    
    # Step 2: Check current checkpoint
    print("\n" + "=" * 70)
    print("STEP 2: Check Current Checkpoint")
    print("=" * 70)
    
    checkpoint_before = check_checkpoint_file()
    
    # Step 3: Get current metrics count
    print("\n" + "=" * 70)
    print("STEP 3: Get Current Metrics")
    print("=" * 70)
    
    current_metrics = api.get_metrics()
    
    # Step 4: Insert new metric
    print("\n" + "=" * 70)
    print("STEP 4: Insert New Health Metric")
    print("=" * 70)
    
    insert_time = datetime.now()
    print(f"ğŸ• Insert timestamp: {insert_time.isoformat()}")
    
    result = api.insert_metric()
    
    if not result:
        print("\nâŒ Test failed - could not insert metric")
        return False
    
    # Step 5: Display next steps
    print("\n" + "=" * 70)
    print("STEP 5: Monitor Spark Logs")
    print("=" * 70)
    
    batch_interval = int(os.getenv('BATCH_INTERVAL_SECONDS', '60'))
    next_batch_time = insert_time + timedelta(seconds=batch_interval)
    
    print(f"\nâ±ï¸  Batch interval: {batch_interval} seconds")
    print(f"ğŸ“… Metric inserted at: {insert_time.strftime('%H:%M:%S')}")
    print(f"ğŸ“… Next batch expected around: {next_batch_time.strftime('%H:%M:%S')}")
    
    print(f"\nğŸ” Watch your Spark terminal for:")
    print(f"   1. ğŸ”„ Starting micro-batch [N]")
    print(f"   2. âœ… Found 1 new/updated records")
    print(f"   3. ğŸ“ˆ Processing 1 health metrics records...")
    print(f"   4. ğŸ’¾ Updating checkpoint to: {insert_time.strftime('%Y-%m-%dT%H:%M')}...")
    
    print(f"\nâ³ Estimated wait time: ~{batch_interval} seconds")
    
    # Step 6: Checkpoint verification instructions
    print("\n" + "=" * 70)
    print("STEP 6: Verify Checkpoint Resume (After Spark Processes)")
    print("=" * 70)
    
    print("\nğŸ“ After you see the metric processed in Spark logs:")
    print("   1. Press Ctrl+C to stop the Spark job")
    print("   2. Check checkpoint file:")
    print("      cat .\\spark-checkpoints\\last_processed_timestamp.txt")
    print("   3. Restart Spark job:")
    print("      python main.py")
    print("   4. Verify it shows:")
    print("      ğŸ“ Loaded checkpoint timestamp: [recent timestamp]")
    print("   5. Next batch should find no new data (if you didn't insert more)")
    
    print("\n" + "=" * 70)
    print("âœ… TEST SETUP COMPLETE")
    print("=" * 70)
    
    print("\nğŸ“Š Summary:")
    print(f"   âœ… Authenticated as: {TEST_EMAIL}")
    print(f"   âœ… Inserted metric at: {insert_time.isoformat()}")
    print(f"   âœ… Checkpoint location: {os.getenv('CHECKPOINT_LOCATION', './spark-checkpoints')}")
    print(f"   âœ… Backend API: {BACKEND_URL}")
    
    print("\nğŸ¯ Next Steps:")
    print("   1. Keep your Spark job running (python main.py)")
    print("   2. Wait for next batch to process the new metric")
    print("   3. Test stop/restart to verify checkpoint resume")
    
    return True


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ§ª SPARK MICRO-BATCH POLLING - INTEGRATION TEST")
    print("=" * 70)
    print(f"ğŸ“… Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    try:
        success = run_integration_test()
        
        if success:
            print("\n\nâœ… Integration test setup completed successfully!")
            print("ğŸ“Š Monitor your Spark terminal for processing logs")
        else:
            print("\n\nâŒ Integration test setup failed")
            
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Test interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Test error: {e}")
        import traceback
        traceback.print_exc()
