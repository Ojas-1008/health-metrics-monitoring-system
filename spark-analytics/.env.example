# ============================================================================
# Spark Analytics Configuration
# Health Metrics Monitoring System - Analytics Processing Engine
# ============================================================================

# ------------------------------------------------------------------------------
# MongoDB Configuration
# ------------------------------------------------------------------------------
# MongoDB connection URI (Atlas or local)
# Format (Atlas): mongodb+srv://username:password@cluster.mongodb.net/database
# Format (Local): mongodb://localhost:27017/database
MONGO_URI=mongodb://localhost:27017/health-metrics

# Database name (must match backend server database)
MONGO_DB_NAME=health-metrics

# Collections
HEALTHMETRICS_COLLECTION=healthmetrics
ANALYTICS_COLLECTION=analytics
USERS_COLLECTION=users

# ------------------------------------------------------------------------------
# Apache Spark Configuration
# ------------------------------------------------------------------------------
# Spark application name (appears in Spark UI)
SPARK_APP_NAME=HealthMetricsAnalytics

# Spark master URL
# - local[*]: Run locally with all available cores
# - local[4]: Run locally with 4 cores
# - spark://master:7077: Connect to standalone cluster
SPARK_MASTER=local[*]

# Spark executor memory (adjust based on your system)
SPARK_EXECUTOR_MEMORY=2g

# Spark driver memory
SPARK_DRIVER_MEMORY=1g

# ------------------------------------------------------------------------------
# Streaming Configuration
# ------------------------------------------------------------------------------
# Batch interval in seconds (how often to process data)
# Recommended: 60 for 1-minute micro-batches, 30 for faster updates
BATCH_INTERVAL_SECONDS=60

# Processing window for rolling averages (in days)
ROLLING_WINDOW_DAYS=7

# Anomaly detection sensitivity (1.5 = moderate, 2.0 = less sensitive)
ANOMALY_THRESHOLD=1.5

# ------------------------------------------------------------------------------
# Checkpoint Configuration
# ------------------------------------------------------------------------------
# Directory for Spark streaming checkpoints (ensures fault tolerance)
CHECKPOINT_LOCATION=./spark-checkpoints

# Enable checkpoint cleanup on job restart (true/false)
CHECKPOINT_CLEANUP_ON_RESTART=false

# ------------------------------------------------------------------------------
# Backend API Configuration
# ------------------------------------------------------------------------------
# Backend API URL for triggering SSE events
BACKEND_API_URL=http://localhost:5000

# API endpoint for notifying analytics updates
ANALYTICS_WEBHOOK_ENDPOINT=/api/analytics/webhook

# API authentication token (if required)
# BACKEND_API_TOKEN=your-api-token-here

# ------------------------------------------------------------------------------
# Logging Configuration
# ------------------------------------------------------------------------------
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log output directory
LOG_DIR=./logs

# Enable colored logs (true/false)
ENABLE_COLOR_LOGS=true

# ------------------------------------------------------------------------------
# Feature Flags
# ------------------------------------------------------------------------------
# Enable anomaly detection processing
ENABLE_ANOMALY_DETECTION=true

# Enable trend analysis
ENABLE_TREND_ANALYSIS=true

# Enable goal predictions
ENABLE_GOAL_PREDICTIONS=true

# Enable MongoDB change stream monitoring (for real-time processing)
ENABLE_CHANGE_STREAM=true

# ------------------------------------------------------------------------------
# Performance Tuning
# ------------------------------------------------------------------------------
# Maximum records to process per batch
MAX_RECORDS_PER_BATCH=1000

# MongoDB read preference (primary, secondary, nearest)
MONGO_READ_PREFERENCE=primary

# Number of partitions for Spark processing
SPARK_PARTITIONS=4

# Enable Spark UI (true/false) - runs on port 4040
ENABLE_SPARK_UI=true

# ------------------------------------------------------------------------------
# Development/Testing Configuration
# ------------------------------------------------------------------------------
# Enable test mode (processes sample data only)
TEST_MODE=false

# Sample user ID for testing (leave empty for production)
TEST_USER_ID=

# Enable verbose debug output
DEBUG_MODE=false