# Phone-Only Metric Aggregations - Implementation Guide

## Overview

The analytics processing system computes comprehensive insights from health metrics data while enforcing phone-only constraints. This ensures compatibility with the system's design philosophy of not requiring wearable devices.

## âœ… Implementation Complete

All analytics features have been implemented:

1. âœ… Phone-only metric filtering
2. âœ… 7-day rolling averages
3. âœ… Activity streak tracking
4. âœ… Anomaly detection (2Ïƒ threshold)
5. âœ… Percentile rankings (90-day window)
6. âœ… Trend analysis
7. âœ… MongoDB analytics storage

## Analytics Pipeline

### Step 1: Phone-Only Filtering

**Purpose**: Enforce wearable-free constraint

```python
# Filter by source
phone_only_df = df.filter(col('source').isin(['manual', 'googlefit']))

# Drop wearable-only fields
if 'heartRate' in schema_fields:
    phone_only_df = phone_only_df.filter(col('heartRate').isNull())
if 'bloodOxygen' in schema_fields:
    phone_only_df = phone_only_df.filter(col('bloodOxygen').isNull())
```

**Supported Metrics**:
- âœ… Steps
- âœ… Distance
- âœ… Calories
- âœ… Active Minutes
- âœ… Weight
- âœ… Sleep Hours

**Excluded (Wearable-Only)**:
- âŒ Heart Rate
- âŒ Blood Oxygen (SpO2)

### Step 2: Data Transformation

**Purpose**: Convert nested structure to long format for analytics

```python
# Flatten nested metrics
metrics_long = df.select(
    'userId', 'date', 'source', 'updatedAt',
    explode(array(
        struct(lit('steps'), col('metrics.steps')),
        struct(lit('distance'), col('metrics.distance')),
        # ... other metrics
    ))
)
```

**Output Format**:
```
userId | date       | metricType | value | source
-------|------------|------------|-------|--------
user1  | 2025-11-20 | steps      | 8500  | manual
user1  | 2025-11-20 | distance   | 6.2   | manual
user1  | 2025-11-20 | calories   | 450   | manual
```

### Step 3: Rolling Averages (7-Day Window)

**Purpose**: Smooth out daily variations and identify trends

```python
window_7day = Window.partitionBy('userId', 'metricType') \
    .orderBy('date') \
    .rowsBetween(-6, 0)  # Last 7 days including current

rolling_avg = avg('value').over(window_7day)
```

**Applied To**: Steps, Distance, Calories, Active Minutes

**Example**:
```
Date       | Steps | 7-Day Avg
-----------|-------|----------
2025-11-14 | 7000  | 7000
2025-11-15 | 8000  | 7500
2025-11-16 | 9000  | 8000
2025-11-17 | 8500  | 8125
2025-11-18 | 7500  | 8000
2025-11-19 | 9500  | 8250
2025-11-20 | 8500  | 8286
```

### Step 4: Activity Streak Tracking

**Purpose**: Identify consecutive days with meaningful activity (steps > 1000)

**Algorithm**:
1. Mark days with steps > 1000 as "streak days"
2. Check if previous day was also a streak day
3. Reset streak counter when gap detected
4. Calculate current streak length

```python
window_streak = Window.partitionBy('userId').orderBy('date')

# Identify streak days
is_streak_day = when(col('steps') > 1000, 1).otherwise(0)

# Check consecutive
is_consecutive = when(
    (datediff(date, prev_date) == 1) &
    (prev_steps > 1000) &
    (steps > 1000),
    1
).otherwise(0)

# Reset counter
streak_reset = when(
    (is_streak_day == 1) & (is_consecutive == 0),
    1
).otherwise(0)
```

**Example**:
```
Date       | Steps | Streak Day | Streak Length
-----------|-------|------------|-------------
2025-11-14 | 1200  | Yes        | 1
2025-11-15 | 1500  | Yes        | 2
2025-11-16 | 1800  | Yes        | 3
2025-11-17 | 800   | No         | 0
2025-11-18 | 1100  | Yes        | 1
2025-11-19 | 1300  | Yes        | 2
2025-11-20 | 1400  | Yes        | 3
```

### Step 5: Anomaly Detection (30-Day Window)

**Purpose**: Flag unusual values that deviate significantly from normal patterns

**Statistical Method**: 2 Standard Deviations (Ïƒ) from mean

```python
window_30day = Window.partitionBy('userId', 'metricType') \
    .orderBy('date') \
    .rowsBetween(-29, 0)  # Last 30 days

# Calculate statistics
rolling_mean = avg('value').over(window_30day)
rolling_stddev = stddev('value').over(window_30day)

# Detect anomalies
anomaly_detected = when(
    abs(value - rolling_mean) > 2 * rolling_stddev,
    True
).otherwise(False)
```

**Interpretation**:
- ~95% of normal values fall within 2Ïƒ
- Values beyond 2Ïƒ are flagged as anomalies
- Could indicate: data errors, unusual activity, health changes

**Example**:
```
Date       | Steps | 30-Day Mean | Std Dev | Anomaly
-----------|-------|-------------|---------|--------
2025-11-15 | 8000  | 8200        | 500     | No
2025-11-16 | 8500  | 8200        | 500     | No
2025-11-17 | 12000 | 8200        | 500     | Yes (>2Ïƒ)
2025-11-18 | 8300  | 8300        | 550     | No
```

### Step 6: Percentile Rankings (90-Day Window)

**Purpose**: Show where current performance ranks historically

**Method**: Rank-based percentile calculation

```python
window_90day = Window.partitionBy('userId', 'metricType') \
    .orderBy('date') \
    .rowsBetween(-89, 0)  # Last 90 days

# Calculate percentile (0.0 to 1.0)
percentile = (row_number() - 1) / (count() - 1)
```

**Interpretation**:
- 0.0 = Lowest value in 90-day window
- 0.5 = Median performance
- 1.0 = Highest value in 90-day window

**Example**:
```
Steps: 8500
90-Day Range: 5000 to 12000
Percentile: 0.65 (65th percentile - better than 65% of days)
```

### Step 7: Trend Analysis

**Purpose**: Compare current performance to previous period

**Method**: Percentage change from previous 7-day average

```python
# Previous 7 days (days -13 to -7 from current)
window_prev_7day = Window.partitionBy('userId', 'metricType') \
    .orderBy('date') \
    .rowsBetween(-13, -7)

prev_avg = avg('value').over(window_prev_7day)

# Calculate percentage change
comparison = ((current_avg - prev_avg) / prev_avg) * 100

# Classify trend
trend = when(comparison > 5, 'increasing')
    .when(comparison < -5, 'decreasing')
    .otherwise('stable')
```

**Thresholds**:
- **Increasing**: > +5% improvement
- **Decreasing**: > -5% decline
- **Stable**: Within Â±5%

**Example**:
```
Current 7-Day Avg: 8500 steps
Previous 7-Day Avg: 8000 steps
Change: +6.25%
Trend: INCREASING
```

### Step 8: Output Structure

**Analytics Record Format**:

```json
{
  "userId": "673d123abc...",
  "metricType": "steps",
  "timeRange": "7-day",
  "date": "2025-11-20",
  "currentValue": 8500,
  "rollingAverage7Day": 8286,
  "trend": "increasing",
  "anomalyDetected": false,
  "streakDays": 3,
  "percentile": 0.65,
  "comparisonToPrevious": 6.25,
  "calculatedAt": "2025-11-20T22:30:00",
  "expiresAt": "2026-02-18T22:30:00"
}
```

**Fields Explained**:
- `userId`: User identifier
- `metricType`: Type of metric (steps, distance, etc.)
- `timeRange`: Window size for rolling average (7-day)
- `date`: Date of the measurement
- `currentValue`: Raw metric value
- `rollingAverage7Day`: 7-day moving average
- `trend`: Direction of change (increasing/decreasing/stable)
- `anomalyDetected`: Statistical outlier flag
- `streakDays`: Consecutive active days (steps only)
- `percentile`: Ranking within 90-day history (0-1)
- `comparisonToPrevious`: % change from previous period
- `calculatedAt`: When analytics were computed
- `expiresAt`: When to recompute (90 days later)

## Window Specifications

| Calculation | Window Size | Purpose |
|-------------|-------------|---------|
| Rolling Average | 7 days | Short-term trend |
| Anomaly Detection | 30 days | Statistical baseline |
| Percentile Rank | 90 days | Historical context |
| Trend Comparison | 14 days (2x7) | Week-over-week change |
| Streak Tracking | All history | Cumulative achievement |

## Performance Considerations

### Optimization Strategies

1. **Data Partitioning**
   ```python
   # Partition by userId and metricType
   Window.partitionBy('userId', 'metricType')
   ```
   - Enables parallel processing per user
   - Reduces shuffle operations

2. **Caching**
   ```python
   new_data.cache()  # Cache input data
   # ... process ...
   new_data.unpersist()  # Free memory
   ```

3. **Filtering Early**
   ```python
   # Filter to phone-only before processing
   phone_only_df = df.filter(col('source').isin(['manual', 'googlefit']))
   ```

4. **Efficient Windows**
   ```python
   # Use rowsBetween for bounded windows
   .rowsBetween(-6, 0)  # More efficient than unbounded
   ```

### Expected Performance

| Data Volume | Processing Time | Memory Usage |
|-------------|----------------|--------------|
| 100 records | < 10 seconds | 500 MB |
| 1,000 records | < 30 seconds | 1 GB |
| 10,000 records | < 2 minutes | 2 GB |

## Testing

### Run Analytics Tests

```powershell
cd spark-analytics
python test_analytics.py
```

**Tests Include**:
1. âœ… Analytics schema validation
2. âœ… Phone-only filtering
3. âœ… Streak calculation logic
4. âœ… Analytics collection access

### Manual Testing

```powershell
# 1. Start streaming job
python main.py

# 2. Insert test metric (another terminal)
python test_streaming_integration.py

# 3. Wait for batch processing (~60 seconds)
# Watch for analytics output in logs

# 4. Verify analytics collection
python test_analytics.py
```

## Monitoring Analytics

### Check Analytics Collection

```python
from pymongo import MongoClient

client = MongoClient(MONGO_URI)
db = client[MONGO_DB_NAME]
analytics = db['analytics']

# Count analytics records
print(f"Total analytics: {analytics.count_documents({})}")

# By metric type
pipeline = [
    {'$group': {'_id': '$metricType', 'count': {'$sum': 1}}}
]
print(list(analytics.aggregate(pipeline)))

# Recent anomalies
anomalies = analytics.find({'anomalyDetected': True}).limit(10)
for a in anomalies:
    print(f"{a['userId']} - {a['metricType']}: {a['currentValue']}")
```

### Spark Logs to Monitor

```
ðŸ“Š Processing health metrics batch...
ðŸ” Step 1: Filtering to phone-supported metrics...
   âœ… Filtered to 150 phone-only metric records
ðŸ”„ Step 2: Flattening metrics structure...
   âœ… Flattened to 600 individual metric values
ðŸ“ˆ Step 3: Computing 7-day rolling averages...
   âœ… 7-day rolling averages computed
ðŸ”¥ Step 4: Computing activity streaks...
   âœ… Activity streaks computed
âš ï¸ Step 5: Computing 30-day statistics for anomaly detection...
   âœ… Anomaly detection completed
ðŸ“Š Step 6: Computing percentile rankings (90-day window)...
   âœ… Percentile rankings computed
ðŸ“ˆ Step 7: Computing trends...
   âœ… Trends computed
ðŸ“¦ Step 8: Structuring analytics output...
   âœ… Analytics output structured (450 records)
ðŸ’¾ Saving analytics to MongoDB...
   âœ… Saved 450 analytics records to MongoDB
```

## Troubleshooting

### Issue: No analytics generated

**Causes**:
1. Not enough historical data (< 7 days)
2. All metrics filtered out (wearable-only)
3. Missing required fields

**Solution**:
```python
# Check data availability
df.groupBy('source').count().show()
df.filter(col('date') >= '2025-11-13').count()
```

### Issue: Streak calculation incorrect

**Debug**:
```python
# Check steps data
steps_df = df.filter(col('metricType') == 'steps')
steps_df.orderBy('date').show(20)

# Verify date continuity
steps_df.select(
    'date',
    lag('date', 1).over(Window.orderBy('date')).alias('prev_date'),
    datediff('date', lag('date', 1).over(Window.orderBy('date'))).alias('gap')
).show()
```

### Issue: Anomalies not detected

**Causes**:
1. Insufficient data (< 30 days)
2. Low variance in data
3. No extreme values

**Check**:
```python
# View distribution
df.describe('value').show()

# Check standard deviation
from pyspark.sql.functions import stddev
df.groupBy('metricType').agg(stddev('value')).show()
```

### Issue: MongoDB write fails

**Solution**:
```python
# Verify connection
spark.read.format("mongodb") \
    .option("database", MONGO_DB_NAME) \
    .option("collection", "analytics") \
    .load().count()

# Check write permissions
# Ensure user has write access to analytics collection
```

## Next Steps

After successful implementation:

1. âœ… Run integration tests
2. âœ… Verify analytics in MongoDB
3. âœ… Monitor batch processing
4. âœ… Set up dashboard visualizations
5. âœ… Configure alerts for anomalies
6. âœ… Optimize for production scale

## API Integration

Analytics can be consumed by the backend API:

```javascript
// Backend endpoint to fetch user analytics
GET /api/analytics/:userId?metricType=steps&timeRange=7-day

// Response
{
  "success": true,
  "data": {
    "analytics": [{
      "metricType": "steps",
      "currentValue": 8500,
      "rollingAverage7Day": 8286,
      "trend": "increasing",
      "streakDays": 3,
      "percentile": 0.65
    }]
  }
}
```

## References

- **Implementation**: [main.py](./main.py) - `process_health_metrics()` function
- **Tests**: [test_analytics.py](./test_analytics.py)
- **Architecture**: [POLLING_ARCHITECTURE.md](./POLLING_ARCHITECTURE.md)
- **Integration Tests**: [INTEGRATION_TEST_GUIDE.md](./INTEGRATION_TEST_GUIDE.md)
