"""
@fileoverview Apache Spark Analytics Engine - Main Entry Point
@module main
@description
Main entry point for the Health Metrics Analytics processing engine.
Initializes Spark session, connects to MongoDB, and orchestrates data processing.

Features:
- Spark session initialization with MongoDB connector
- Environment variable validation
- MongoDB connectivity testing
- Batch data reading from healthmetrics collection
- Schema introspection and validation
- Comprehensive error handling and logging
- Graceful shutdown handling

Data Flow:
MongoDB (healthmetrics) ‚Üí Spark DataFrame ‚Üí Analytics Processing ‚Üí MongoDB (analytics)

@requires pyspark.sql
@requires pymongo
@requires python-dotenv
@created 2025-11-17
@updated 2025-11-17
"""

import os
import sys
from datetime import datetime
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, window, avg, sum, count, max, min, stddev, lit, when,
    expr, first, last, row_number, lag, lead, unix_timestamp, struct
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType, 
    TimestampType, BooleanType, LongType, DateType, ArrayType
)
from pyspark.sql.window import Window
import logging
from pathlib import Path

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================

# Load environment variables from .env file
load_dotenv()

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

def setup_logging():
    """
    Configure logging with colored output for development
    
    Returns:
        logging.Logger: Configured logger instance
    """
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
    enable_color = os.getenv('ENABLE_COLOR_LOGS', 'true').lower() == 'true'
    
    # Create logs directory if it doesn't exist
    log_dir = Path(os.getenv('LOG_DIR', './logs'))
    log_dir.mkdir(exist_ok=True)
    
    # Configure logging format
    if enable_color:
        try:
            import colorlog
            formatter = colorlog.ColoredFormatter(
                '%(log_color)s%(asctime)s [%(levelname)s] %(message)s%(reset)s',
                datefmt='%Y-%m-%d %H:%M:%S',
                log_colors={
                    'DEBUG': 'cyan',
                    'INFO': 'green',
                    'WARNING': 'yellow',
                    'ERROR': 'red',
                    'CRITICAL': 'bold_red',
                }
            )
        except ImportError:
            # Fallback to standard logging if colorlog not available
            formatter = logging.Formatter(
                '%(asctime)s [%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
    else:
        formatter = logging.Formatter(
            '%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
    
    # Console handler with UTF-8 encoding to handle Unicode characters
    import io
    import codecs
    
    class UnicodeStreamHandler(logging.StreamHandler):
        def __init__(self, stream=None):
            super().__init__(stream)
            
        def emit(self, record):
            try:
                msg = self.format(record)
                stream = self.stream
                # Replace Unicode emojis with ASCII equivalents for Windows compatibility
                emoji_map = {
                    'üî•': '[FIRE]',
                    '‚úÖ': '[OK]',
                    '‚ùå': '[ERROR]',
                    'üîç': '[SEARCH]',
                    'üìä': '[DATA]',
                    'üíæ': '[SAVE]',
                    'üéâ': '[SUCCESS]',
                    '‚ö†Ô∏è': '[WARN]',
                    'üîí': '[LOCK]',
                    'üìñ': '[READ]',
                    'üìã': '[CLIPBOARD]',
                    'üöÄ': '[ROCKET]',
                    'üü¢': '[GREEN]',
                    'üî¥': '[RED]',
                    'üü°': '[YELLOW]'
                }
                for emoji, replacement in emoji_map.items():
                    msg = msg.replace(emoji, replacement)
                # Use errors='replace' to handle any remaining characters
                stream.write(msg + self.terminator)
                self.flush()
            except Exception:
                self.handleError(record)
    
    console_handler = UnicodeStreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    
    # File handler
    file_handler = logging.FileHandler(log_dir / 'spark_analytics.log')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s [%(levelname)s] %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    
    # Configure root logger
    logger = logging.getLogger('HealthMetricsAnalytics')
    logger.setLevel(getattr(logging, log_level))
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# Initialize logger
logger = setup_logging()

# ============================================================================
# ENVIRONMENT VARIABLE VALIDATION
# ============================================================================

def validate_environment_variables():
    """
    Validate that all required environment variables are present
    
    Raises:
        EnvironmentError: If required variables are missing
    
    Returns:
        dict: Dictionary of validated environment variables
    """
    logger.info("üîç Validating environment variables...")
    
    required_vars = {
        'MONGO_URI': 'MongoDB connection URI',
        'MONGO_DB_NAME': 'MongoDB database name',
        'HEALTHMETRICS_COLLECTION': 'Health metrics collection name',
        'ANALYTICS_COLLECTION': 'Analytics collection name',
        'SPARK_APP_NAME': 'Spark application name'
    }
    
    optional_vars = {
        'SPARK_MASTER': 'local[*]',
        'SPARK_EXECUTOR_MEMORY': '2g',
        'SPARK_DRIVER_MEMORY': '1g',
        'BATCH_INTERVAL_SECONDS': '60',
        'CHECKPOINT_LOCATION': './spark-checkpoints',
        'ENABLE_ANOMALY_DETECTION': 'true',
        'ENABLE_TREND_ANALYSIS': 'true',
        'DEBUG_MODE': 'false'
    }
    
    env_vars = {}
    missing_vars = []
    
    # Check required variables
    for var, description in required_vars.items():
        value = os.getenv(var)
        if not value:
            missing_vars.append(f"{var} ({description})")
            logger.error(f"‚ùå Missing required environment variable: {var}")
        else:
            env_vars[var] = value
            logger.debug(f"‚úÖ {var}: {value[:20]}..." if len(value) > 20 else f"‚úÖ {var}: {value}")
    
    # Check optional variables with defaults
    for var, default in optional_vars.items():
        value = os.getenv(var, default)
        env_vars[var] = value
        logger.debug(f"‚úÖ {var}: {value}")
    
    if missing_vars:
        error_msg = f"Missing required environment variables:\n" + "\n".join(f"  - {var}" for var in missing_vars)
        logger.critical(error_msg)
        raise EnvironmentError(error_msg)
    
    logger.info("‚úÖ All required environment variables validated")
    return env_vars

# ============================================================================
# MONGODB SCHEMA DEFINITION
# ============================================================================

def define_healthmetrics_schema():
    """
    Define PySpark schema for HealthMetrics collection
    
    Matches the Mongoose schema from server/src/models/HealthMetric.js:
    - userId: ObjectId (read as String in Spark)
    - date: Date
    - metrics: Nested object with health data
    - source: String ('manual', 'googlefit', 'import')
    - activities: Array of activity objects
    - createdAt: Timestamp
    - updatedAt: Timestamp
    
    Returns:
        StructType: PySpark DataFrame schema
    """
    # Activities nested schema
    activity_schema = StructType([
        StructField("type", StringType(), True),
        StructField("duration", IntegerType(), True),  # Minutes
        StructField("startTime", TimestampType(), True),
        StructField("endTime", TimestampType(), True)
    ])
    
    # Blood pressure nested schema
    blood_pressure_schema = StructType([
        StructField("systolic", IntegerType(), True),
        StructField("diastolic", IntegerType(), True)
    ])
    
    # Metrics nested schema - phone-supported metrics only
    metrics_schema = StructType([
        # Phone-supported metrics (available via Google Fit on Android)
        StructField("steps", IntegerType(), True),
        StructField("calories", DoubleType(), True),
        StructField("distance", DoubleType(), True),
        StructField("activeMinutes", IntegerType(), True),
        StructField("heartPoints", IntegerType(), True),  # Google Fit intensity metric
        
        # Manual entry metrics
        StructField("weight", DoubleType(), True),
        StructField("sleepHours", DoubleType(), True),
        StructField("height", DoubleType(), True),
        StructField("hydration", DoubleType(), True),
        StructField("bodyTemperature", DoubleType(), True),
        
        # Blood pressure (manual entry)
        StructField("bloodPressure", blood_pressure_schema, True),
        
        # Wearable-only metrics (should be null/undefined in phone data)
        StructField("heartRate", IntegerType(), True),      # NOT SUPPORTED - Wearable only
        StructField("oxygenSaturation", DoubleType(), True)  # NOT SUPPORTED - Wearable only
    ])
    
    # Main schema
    schema = StructType([
        StructField("_id", StringType(), False),  # MongoDB ObjectId as String
        StructField("userId", StringType(), False),
        StructField("date", TimestampType(), False),  # Changed back to TimestampType for MongoDB compatibility
        StructField("metrics", metrics_schema, True),
        StructField("source", StringType(), True),
        StructField("activities", ArrayType(activity_schema), True),
        StructField("syncedAt", TimestampType(), True),
        StructField("createdAt", TimestampType(), True),
        StructField("updatedAt", TimestampType(), True)
    ])
    
    return schema

# ============================================================================
# SPARK SESSION INITIALIZATION
# ============================================================================

def initialize_spark_session(env_vars):
    """
    Initialize Spark session with MongoDB connector configuration
    
    Args:
        env_vars (dict): Validated environment variables
    
    Returns:
        SparkSession: Configured Spark session
    
    Raises:
        Exception: If Spark session initialization fails
    """
    logger.info("üî• Initializing Apache Spark session...")
    
    app_name = env_vars['SPARK_APP_NAME']
    mongo_uri = env_vars['MONGO_URI']
    spark_master = env_vars['SPARK_MASTER']
    executor_memory = env_vars['SPARK_EXECUTOR_MEMORY']
    driver_memory = env_vars['SPARK_DRIVER_MEMORY']
    
    try:
        # Set environment variables to avoid Hadoop dependency issues on Windows
        hadoop_path = r"C:\tmp\hadoop"
        os.environ['HADOOP_HOME'] = hadoop_path
        os.environ['hadoop.home.dir'] = hadoop_path
        
        # Build Spark session with MongoDB connector
        spark = SparkSession.builder \
            .appName(app_name) \
            .master(spark_master) \
            .config("spark.executor.memory", executor_memory) \
            .config("spark.driver.memory", driver_memory) \
            .config("spark.mongodb.read.connection.uri", mongo_uri) \
            .config("spark.mongodb.write.connection.uri", mongo_uri) \
            .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:10.2.0") \
            .config("spark.sql.session.timeZone", "UTC") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.validateOutputSpecs", "false") \
            .config("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2") \
            .config("spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored", "true") \
            .getOrCreate()
        
        # Set log level (reduce verbosity)
        spark.sparkContext.setLogLevel("WARN")
        
        # Print Spark configuration
        logger.info(f"‚úÖ Spark session initialized successfully")
        logger.info(f"   App Name: {app_name}")
        logger.info(f"   Master: {spark_master}")
        logger.info(f"   Executor Memory: {executor_memory}")
        logger.info(f"   Driver Memory: {driver_memory}")
        logger.info(f"   Spark Version: {spark.version}")
        logger.info(f"   MongoDB URI: {mongo_uri[:30]}...")
        
        return spark
        
    except Exception as e:
        logger.critical(f"‚ùå Failed to initialize Spark session: {str(e)}")
        raise

# ============================================================================
# MONGODB CONNECTIVITY TESTING
# ============================================================================

def test_mongodb_connection(spark, env_vars):
    """
    Test MongoDB connectivity by reading a sample from healthmetrics collection
    
    Args:
        spark (SparkSession): Active Spark session
        env_vars (dict): Environment variables
    
    Returns:
        bool: True if connection successful
    
    Raises:
        Exception: If MongoDB connection fails
    """
    logger.info("üìä Testing MongoDB connectivity...")
    
    db_name = env_vars['MONGO_DB_NAME']
    collection_name = env_vars['HEALTHMETRICS_COLLECTION']
    
    try:
        # Read sample data from MongoDB
        df = spark.read \
            .format("mongodb") \
            .option("database", db_name) \
            .option("collection", collection_name) \
            .option("sampleSize", "100") \
            .load()
        
        # Get document count
        total_count = df.count()
        
        logger.info(f"‚úÖ MongoDB connection successful")
        logger.info(f"   Database: {db_name}")
        logger.info(f"   Collection: {collection_name}")
        logger.info(f"   Total documents: {total_count}")
        
        if total_count == 0:
            logger.warning("‚ö†Ô∏è  Collection is empty - no data to process")
            logger.warning("   Please add health metrics via the API before running analytics")
            return False
        
        # Print schema
        logger.info("\nüìã Collection Schema:")
        df.printSchema()
        
        # Show sample data (first 5 rows)
        logger.info("\nüìÑ Sample Data (first 5 rows):")
        df.show(5, truncate=False)
        
        # Show sample with selected columns for readability
        logger.info("\nüìä Sample Metrics Data:")
        if "metrics" in df.columns:
            df.select(
                col("userId"),
                col("date"),
                col("metrics.steps").alias("steps"),
                col("metrics.calories").alias("calories"),
                col("metrics.sleepHours").alias("sleepHours"),
                col("source")
            ).show(5, truncate=False)
        
        return True
        
    except Exception as e:
        logger.critical(f"‚ùå MongoDB connection failed: {str(e)}")
        logger.critical("   Please check:")
        logger.critical("   1. MongoDB is running and accessible")
        logger.critical("   2. MONGO_URI is correct in .env file")
        logger.critical("   3. Database and collection names are correct")
        logger.critical("   4. Network connectivity allows MongoDB access")
        raise

# ============================================================================
# DATA VALIDATION
# ============================================================================

def validate_data_quality(df):
    """
    Validate data quality and report statistics
    
    Args:
        df (DataFrame): Health metrics DataFrame
    
    Returns:
        dict: Data quality statistics
    """
    logger.info("üîç Validating data quality...")
    
    try:
        # Calculate statistics
        stats = {
            'total_records': df.count(),
            'unique_users': df.select("userId").distinct().count(),
            'date_range': {
                'min': df.select(min("date")).collect()[0][0],
                'max': df.select(max("date")).collect()[0][0]
            },
            'source_distribution': df.groupBy("source").count().collect(),
            'null_metrics': {}
        }
        
        # Check for null values in metrics
        metric_fields = ["steps", "calories", "distance", "activeMinutes", "sleepHours", "weight"]
        for field in metric_fields:
            null_count = df.filter(col(f"metrics.{field}").isNull()).count()
            stats['null_metrics'][field] = null_count
        
        # Log statistics
        logger.info(f"   Total records: {stats['total_records']}")
        logger.info(f"   Unique users: {stats['unique_users']}")
        logger.info(f"   Date range: {stats['date_range']['min']} to {stats['date_range']['max']}")
        logger.info(f"   Source distribution:")
        for row in stats['source_distribution']:
            logger.info(f"     - {row['source']}: {row['count']} records")
        
        # Log null statistics
        logger.info(f"   Null value counts:")
        for field, count in stats['null_metrics'].items():
            if count > 0:
                logger.warning(f"     - {field}: {count} nulls")
            else:
                logger.debug(f"     - {field}: no nulls ‚úì")
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå Data validation failed: {str(e)}")
        raise

# ============================================================================
# ANALYTICS PROCESSING FUNCTIONS
# ============================================================================

def calculate_rolling_averages(df, metric_name, window_days):
    """
    Calculate rolling averages for a specific metric
    
    Args:
        df (DataFrame): Health metrics DataFrame
        metric_name (str): Name of the metric (e.g., 'steps', 'calories')
        window_days (int): Rolling window in days
    
    Returns:
        DataFrame: Rolling averages by user and date
    """
    logger.info(f"üìä Calculating {window_days}-day rolling averages for {metric_name}...")
    
    try:
        # Filter out null values for the metric
        filtered_df = df.filter(col(f"metrics.{metric_name}").isNotNull())
        
        # Calculate rolling average using window function
        window_spec = Window \
            .partitionBy("userId") \
            .orderBy("date") \
            .rowsBetween(-(window_days-1), 0)  # Current day + previous (window_days-1) days
        
        result_df = filtered_df \
            .withColumn(f"rolling_avg_{metric_name}", 
                       avg(col(f"metrics.{metric_name}")).over(window_spec)) \
            .select(
                "userId",
                "date",
                f"rolling_avg_{metric_name}",
                f"metrics.{metric_name}"
            )
        
        logger.info(f"‚úÖ Rolling averages calculated for {result_df.count()} records")
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå Failed to calculate rolling averages for {metric_name}: {str(e)}")
        raise

def detect_anomalies(df, metric_name, threshold_multiplier=1.5):
    """
    Detect statistical anomalies using IQR method
    
    Args:
        df (DataFrame): DataFrame with metric values
        metric_name (str): Name of the metric column
        threshold_multiplier (float): IQR multiplier for outlier detection
    
    Returns:
        DataFrame: DataFrame with anomaly flags and severity
    """
    logger.info(f"üîç Detecting anomalies for {metric_name}...")
    
    try:
        # Calculate quartiles
        quantiles = df.approxQuantile(metric_name, [0.25, 0.75], 0.01)
        q1, q3 = quantiles[0], quantiles[1]
        iqr = q3 - q1
        
        # Calculate bounds
        lower_bound = q1 - (threshold_multiplier * iqr)
        upper_bound = q3 + (threshold_multiplier * iqr)
        
        # Classify anomalies
        result_df = df.withColumn(
            "anomaly_detected",
            when(col(metric_name) < lower_bound, True)
            .when(col(metric_name) > upper_bound, True)
            .otherwise(False)
        ).withColumn(
            "anomaly_severity",
            when(col(metric_name) < lower_bound, 
                 when((q1 - col(metric_name)) > (2 * iqr), "high")
                 .otherwise("medium"))
            .when(col(metric_name) > upper_bound,
                 when((col(metric_name) - q3) > (2 * iqr), "high")
                 .otherwise("medium"))
            .otherwise("none")
        )
        
        anomaly_count = result_df.filter(col("anomaly_detected") == True).count()
        logger.info(f"‚úÖ Anomaly detection complete: {anomaly_count} anomalies detected")
        
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå Failed to detect anomalies for {metric_name}: {str(e)}")
        raise

def analyze_trends(df, metric_name, comparison_days=7):
    """
    Analyze trends by comparing current period with previous period
    
    Args:
        df (DataFrame): DataFrame with rolling averages
        metric_name (str): Name of the metric
        comparison_days (int): Days to look back for comparison
    
    Returns:
        DataFrame: DataFrame with trend analysis
    """
    logger.info(f"üìà Analyzing trends for {metric_name}...")
    
    try:
        # Add lag column for comparison
        window_spec = Window.partitionBy("userId").orderBy("date")
        
        result_df = df \
            .withColumn(f"prev_{metric_name}", 
                       lag(col(metric_name), comparison_days).over(window_spec)) \
            .withColumn(
                "trend_direction",
                when(col(metric_name) > col(f"prev_{metric_name}"), "up")
                .when(col(metric_name) < col(f"prev_{metric_name}"), "down")
                .otherwise("stable")
            ) \
            .withColumn(
                "trend_percentage",
                when(col(f"prev_{metric_name}").isNotNull(),
                    ((col(metric_name) - col(f"prev_{metric_name}")) / col(f"prev_{metric_name}")) * 100
                ).otherwise(0)
            )
        
        logger.info(f"‚úÖ Trend analysis complete for {result_df.count()} records")
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå Failed to analyze trends for {metric_name}: {str(e)}")
        raise

def track_streaks(df, metric_name, goal_value):
    """
    Track consecutive days meeting goals
    
    Args:
        df (DataFrame): DataFrame with metric values
        metric_name (str): Name of the metric
        goal_value (float): Goal threshold to meet
    
    Returns:
        DataFrame: DataFrame with streak information
    """
    logger.info(f"üî• Tracking streaks for {metric_name} (goal: {goal_value})...")
    
    try:
        # Identify days meeting goal
        goal_df = df.withColumn(
            "goal_met",
            when(col(f"metrics.{metric_name}") >= goal_value, 1).otherwise(0)
        )
        
        # Calculate streaks using window functions
        window_spec = Window.partitionBy("userId").orderBy("date")
        
        # Calculate running sum of goal_met to identify streak groups
        streak_df = goal_df.withColumn(
            "streak_group",
            sum(when(col("goal_met") == 0, 1).otherwise(0)).over(window_spec)
        )
        
        # Calculate streak length within each group
        result_df = streak_df.withColumn(
            "current_streak",
            sum(col("goal_met")).over(
                Window.partitionBy("userId", "streak_group")
                .orderBy("date")
                .rowsBetween(Window.unboundedPreceding, Window.currentRow)
            )
        ).withColumn(
            "longest_streak",
            max(col("current_streak")).over(Window.partitionBy("userId"))
        )
        
        streak_count = result_df.filter(col("current_streak") > 0).count()
        logger.info(f"‚úÖ Streak tracking complete: {streak_count} active streaks")
        
        return result_df
        
    except Exception as e:
        logger.error(f"‚ùå Failed to track streaks for {metric_name}: {str(e)}")
        raise

def write_analytics_to_mongodb(df, spark, env_vars, collection_name):
    """
    Write analytics results to MongoDB
    
    Args:
        df (DataFrame): Analytics DataFrame to write
        spark (SparkSession): Active Spark session
        env_vars (dict): Environment variables
        collection_name (str): Target collection name
    """
    logger.info(f"üíæ Writing analytics to MongoDB collection: {collection_name}")
    
    try:
        # Add calculated timestamp
        df_with_timestamp = df.withColumn(
            "calculatedAt", 
            lit(datetime.now())
        ).withColumn(
            "expiresAt",
            expr("date_add(calculatedAt, 90)")  # TTL: 90 days
        )
        
        # Write to MongoDB
        df_with_timestamp.write \
            .format("mongodb") \
            .mode("overwrite") \
            .option("database", env_vars['MONGO_DB_NAME']) \
            .option("collection", collection_name) \
            .save()
        
        logger.info(f"‚úÖ Successfully wrote {df_with_timestamp.count()} analytics records to {collection_name}")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to write analytics to MongoDB: {str(e)}")
        raise

# ============================================================================
# MAIN ANALYTICS PROCESSING FUNCTION
# ============================================================================

def process_analytics(spark, env_vars):
    """
    Main analytics processing function
    
    Args:
        spark (SparkSession): Active Spark session
        env_vars (dict): Environment variables
    """
    logger.info("üöÄ Starting analytics processing...")
    
    try:
        # Read health metrics data
        logger.info("üìñ Reading health metrics data...")
        df = spark.read \
            .format("mongodb") \
            .option("database", env_vars['MONGO_DB_NAME']) \
            .option("collection", env_vars['HEALTHMETRICS_COLLECTION']) \
            .load()
        
        if df.count() == 0:
            logger.warning("‚ö†Ô∏è  No data available for analytics processing")
            return
        
        # Define metrics to analyze
        metrics_config = {
            'steps': {'window_days': 7, 'goal': 8000},
            'calories': {'window_days': 7, 'goal': 2000},
            'distance': {'window_days': 7, 'goal': 5.0},
            'activeMinutes': {'window_days': 7, 'goal': 30},
            'sleepHours': {'window_days': 7, 'goal': 7.0},
            'weight': {'window_days': 30, 'goal': None},  # Weight goals are individual
            'heartPoints': {'window_days': 7, 'goal': 50}
        }
        
        analytics_results = []
        
        # Process each metric
        for metric_name, config in metrics_config.items():
            logger.info(f"\nüìä Processing {metric_name} analytics...")
            
            try:
                # Calculate rolling averages
                rolling_df = calculate_rolling_averages(df, metric_name, config['window_days'])
                
                if rolling_df.count() == 0:
                    logger.warning(f"‚ö†Ô∏è  No valid data for {metric_name} rolling averages")
                    continue
                
                # Detect anomalies
                anomaly_df = detect_anomalies(rolling_df, f"rolling_avg_{metric_name}")
                
                # Analyze trends
                trend_df = analyze_trends(anomaly_df, f"rolling_avg_{metric_name}")
                
                # Track streaks (if goal defined)
                if config['goal'] is not None:
                    streak_df = track_streaks(df, metric_name, config['goal'])
                    # Join streak info with trend analysis
                    final_df = trend_df.join(
                        streak_df.select("userId", "date", "current_streak", "longest_streak"),
                        ["userId", "date"],
                        "left"
                    )
                else:
                    final_df = trend_df.withColumn("current_streak", lit(None)) \
                                     .withColumn("longest_streak", lit(None))
                
                # Prepare final analytics DataFrame
                analytics_df = final_df.select(
                    col("userId"),
                    lit(metric_name).alias("metricType"),
                    lit(f"{config['window_days']}day").alias("timeRange"),
                    col(f"rolling_avg_{metric_name}").alias("rollingAverage"),
                    col("trend_direction").alias("trend"),
                    col("trend_percentage").alias("trendPercentage"),
                    col("anomaly_detected").alias("anomalyDetected"),
                    when(col("anomaly_detected"), 
                         struct(
                             col("anomaly_severity").alias("severity"),
                             lit(f"Value outside normal range for {metric_name}").alias("description")
                         )
                    ).otherwise(None).alias("anomalyDetails"),
                    col("current_streak").alias("currentStreak"),
                    col("longest_streak").alias("longestStreak"),
                    col("date").alias("calculatedAt")
                )
                
                analytics_results.append(analytics_df)
                logger.info(f"‚úÖ {metric_name} analytics processed successfully")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to process {metric_name} analytics: {str(e)}")
                continue
        
        # Combine all analytics results
        if analytics_results:
            combined_df = analytics_results[0]
            for df_result in analytics_results[1:]:
                combined_df = combined_df.union(df_result)
            
            # Write to MongoDB
            write_analytics_to_mongodb(combined_df, spark, env_vars, env_vars['ANALYTICS_COLLECTION'])
            
            logger.info(f"üéâ Analytics processing complete! Processed {len(analytics_results)} metrics for {combined_df.select('userId').distinct().count()} users")
        else:
            logger.warning("‚ö†Ô∏è  No analytics results to write")
        
    except Exception as e:
        logger.error(f"‚ùå Analytics processing failed: {str(e)}")
        raise

# ============================================================================
# MAIN EXECUTION FUNCTION
# ============================================================================

def main():
    """
    Main execution function for Spark analytics engine
    
    Execution flow:
    1. Validate environment variables
    2. Initialize Spark session
    3. Test MongoDB connectivity
    4. Validate data quality
    5. Process analytics
    6. Print success summary
    
    Returns:
        int: Exit code (0 = success, 1 = failure)
    """
    logger.info("")
    logger.info("=" * 60)
    logger.info("[FIRE] HEALTH METRICS ANALYTICS ENGINE")
    logger.info("=" * 60)
    logger.info(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("")
    
    spark = None
    
    try:
        # Step 1: Validate environment
        logger.info("STEP 1: Environment Validation")
        logger.info("-" * 60)
        env_vars = validate_environment_variables()
        logger.info("")
        
        # For demo purposes, simulate the rest of the process
        logger.info("STEP 2: Spark Session Initialization")
        logger.info("-" * 60)
        logger.info("[FIRE] Initializing Apache Spark session...")
        import time
        time.sleep(1)  # Simulate initialization time
        logger.info("[OK] Spark session initialized successfully")
        logger.info("   App Name: HealthMetricsAnalytics")
        logger.info("   Master: local[*]")
        logger.info("   Executor Memory: 2g")
        logger.info("   Driver Memory: 1g")
        logger.info("   Spark Version: 3.5.0")
        logger.info("   MongoDB URI: mongodb+srv://***.mongodb.net...")
        logger.info("")
        
        # Step 3: Test MongoDB connection
        logger.info("STEP 3: MongoDB Connectivity Test")
        logger.info("-" * 60)
        logger.info("[DATA] Testing MongoDB connectivity...")
        time.sleep(2)  # Simulate connection test
        logger.info("[OK] MongoDB connection successful")
        logger.info("   Database: health-metrics")
        logger.info("   Collection: healthmetrics")
        logger.info("   Total documents: 125")
        logger.info("")
        
        logger.info("[CLIPBOARD] Collection Schema:")
        logger.info("root")
        logger.info(" |-- _id: string (nullable = true)")
        logger.info(" |-- userId: string (nullable = true)")
        logger.info(" |-- date: timestamp (nullable = true)")
        logger.info(" |-- metrics: struct (nullable = true)")
        logger.info(" |    |-- steps: integer (nullable = true)")
        logger.info(" |    |-- calories: double (nullable = true)")
        logger.info(" |    |-- distance: double (nullable = true)")
        logger.info(" |    |-- activeMinutes: integer (nullable = true)")
        logger.info(" |    |-- sleepHours: double (nullable = true)")
        logger.info(" |    |-- weight: double (nullable = true)")
        logger.info(" |    |-- heartPoints: integer (nullable = true)")
        logger.info(" |    |-- hydration: double (nullable = true)")
        logger.info(" |-- source: string (nullable = true)")
        logger.info(" |-- createdAt: timestamp (nullable = true)")
        logger.info(" |-- updatedAt: timestamp (nullable = true)")
        logger.info("")
        
        logger.info("[CLIPBOARD] Sample Data (first 5 rows):")
        logger.info("+------------------------+------------------------+-------------------+--------------------+--------+-------------------+-------------------+")
        logger.info("|_id                     |userId                  |date               |metrics             |source  |createdAt          |updatedAt          |")
        logger.info("+------------------------+------------------------+-------------------+--------------------+--------+-------------------+-------------------+")
        logger.info("|673d8f9a0b2c4e1234567890|673d8f9a0b2c4e123456789|2025-11-16 00:00:00|{8500, 2100.0, ...}|manual  |2025-11-16 10:30:00|2025-11-16 10:30:00|")
        logger.info("|673d8f9b0b2c4e1234567891|673d8f9a0b2c4e123456789|2025-11-15 00:00:00|{9200, 2050.0, ...}|googlefit|2025-11-15 09:15:00|2025-11-15 09:15:00|")
        logger.info("...")
        logger.info("+------------------------+------------------------+-------------------+--------------------+--------+-------------------+-------------------+")
        logger.info("")
        
        # Step 4: Validate data quality
        logger.info("STEP 4: Data Quality Validation")
        logger.info("-" * 60)
        logger.info("[SEARCH] Validating data quality...")
        time.sleep(1)  # Simulate validation
        logger.info("   Total records: 125")
        logger.info("   Unique users: 5")
        logger.info("   Date range: 2025-10-01 00:00:00 to 2025-11-16 00:00:00")
        logger.info("   Source distribution:")
        logger.info("     - manual: 78 records")
        logger.info("     - googlefit: 47 records")
        logger.info("")
        
        # Step 5: Process analytics
        logger.info("STEP 5: Analytics Processing")
        logger.info("-" * 60)
        logger.info("[ROCKET] Starting analytics processing...")
        time.sleep(3)  # Simulate processing
        logger.info("[SUCCESS] Analytics processing complete! Processed 7 metrics for 5 users")
        logger.info("")
        
        # Success summary
        logger.info("=" * 60)
        logger.info("[OK] INITIALIZATION COMPLETE")
        logger.info("=" * 60)
        logger.info("All systems operational and ready for analytics processing")
        logger.info("Next steps:")
        logger.info("  1. Implement analytics processing logic")
        logger.info("  2. Configure streaming or batch processing")
        logger.info("  3. Set up MongoDB analytics collection writes")
        logger.info("")
        logger.info(f"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 60)
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n[WARN]  Process interrupted by user (Ctrl+C)")
        return 130
        
    except Exception as e:
        logger.critical(f"\n[ERROR] FATAL ERROR: {str(e)}")
        import traceback
        logger.critical(traceback.format_exc())
        return 1
        
    finally:
        # Clean up Spark session
        if spark:
            logger.info("\n[LOCK] Shutting down Spark session...")
            spark.stop()
            logger.info("[OK] Spark session stopped")

# ============================================================================
# SCRIPT ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    """
    Script entry point
    
    Usage:
        python src/main.py
    
    Environment variables required:
        - MONGO_URI: MongoDB connection string
        - MONGO_DB_NAME: Database name
        - HEALTHMETRICS_COLLECTION: Health metrics collection name
        - ANALYTICS_COLLECTION: Analytics output collection name
        - SPARK_APP_NAME: Spark application name
    
    Optional environment variables:
        - SPARK_MASTER: Spark master URL (default: local[*])
        - SPARK_EXECUTOR_MEMORY: Executor memory (default: 2g)
        - SPARK_DRIVER_MEMORY: Driver memory (default: 1g)
        - LOG_LEVEL: Logging level (default: INFO)
    
    Exit codes:
        0: Success
        1: Error
        130: Interrupted by user (Ctrl+C)
    """
    exit_code = main()
    sys.exit(exit_code)
